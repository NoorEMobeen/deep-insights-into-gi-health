{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from Validate import validate_net\n",
    "from Test import test_net\n",
    "from misc import print_metrics, training_curve \n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import logging\n",
    "import csv\n",
    "from torchvision import transforms, datasets, models\n",
    "import sklearn.metrics as mtc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Checking if GPU is used\n",
    "###########################\n",
    "\n",
    "use_cuda=torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "device=torch.device(\"cuda:0\" if use_cuda else \"mps\" if use_mps else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# Setting basic parameters for the model\n",
    "########################################  \n",
    "\n",
    "#Modify these parameters and create so many modals with different configurations\n",
    "         \n",
    "batch_size=32\n",
    "max_epochs=60\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root_dir= \"Train directory path\" # Example: \"../../GastroVision22Aug/train\"\n",
    "val_root_dir= \"Validation data directory path\" # Example: \"../../GastroVision22Aug/val\"\n",
    "test_root_dir= \"Test data directory path\" # Example: \"../../GastroVision22Aug/test\"\n",
    "model_path=r'./checkpoints_{your_batch_size}_{your_epochs}_{your_learning_rate}/'  # set path to the folder that will store model's checkpoints\n",
    "\n",
    "n_classes=22  # number of classes used for training\n",
    "\n",
    "global val_f1_max\n",
    "\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "        os.makedirs(os.path.dirname(model_path))\n",
    "except OSError as err:\n",
    "    print(err)\n",
    "\n",
    "print(\"Directory '% s' created\" % model_path)\n",
    "filename='results_e'+str(max_epochs)+'_'+'b'+str(batch_size)+'_'+'lr'+str(lr)+'_'+'densenet121'   #filename used for saving epoch-wise training details and test results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Training\n",
    "####################################\n",
    "\n",
    "trans={\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4762, 0.3054, 0.2368],\n",
    "                             [0.3345, 0.2407, 0.2164])\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4762, 0.3054, 0.2368],\n",
    "                             [0.3345, 0.2407, 0.2164])\n",
    "    ]),\n",
    "    \n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4762, 0.3054, 0.2368],\n",
    "                             [0.3345, 0.2407, 0.2164])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train:\n",
    "    def __init__(self):\n",
    "\n",
    "        #Generators\n",
    "        training_dataset= datasets.ImageFolder(train_root_dir,transform=trans['train'])\n",
    "        validation_dataset= datasets.ImageFolder(val_root_dir,transform=trans['valid'])\n",
    "        test_dataset= datasets.ImageFolder(test_root_dir,transform=trans['test'])\n",
    "        \n",
    "        self.training_generator=data.DataLoader(training_dataset,batch_size,shuffle=True) # ** unpacks a dictionary into keyword arguments\n",
    "        self.validation_generator=data.DataLoader(validation_dataset,batch_size)\n",
    "        self.test_generator=data.DataLoader(test_dataset,batch_size)\n",
    "       \n",
    "        print('Number of Training set images:{}'.format(len(training_dataset)))\n",
    "        print('Number of Validation set images:{}'.format(len(validation_dataset)))\n",
    "        print('Number of Test set images:{}'.format(len(test_dataset)))\n",
    "        \n",
    "    def train_net(self):\n",
    "        \n",
    "        #Initialize model\n",
    "        model = torchvision.models.densenet121(weights=True).to(device)   # make weights=True if you want to download pre-trained weights\n",
    "        \n",
    "        \n",
    "#         model.load_state_dict(torch.load('./densenet121.pth',map_location='cuda'))   # provide a .pth path for already downloaded weights; otherwise comment this line out\n",
    "        \n",
    "        \n",
    "        # Option to freeze model weights\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True                       # Set param.requires_grad = False if you want to train only the last updated layers and freeze all other layers\n",
    "        \n",
    "        n_inputs = model.classifier.in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "                      nn.Linear(n_inputs, n_classes),                  \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "        \n",
    "       \n",
    "        model.to(device)\n",
    "        optimizer=optim.Adam(model.parameters(), lr, weight_decay=1e-4)\n",
    "        scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',patience=4,verbose=True)\n",
    "        criterion = nn.NLLLoss()\n",
    "        val_f1_max=0.0\n",
    "        epochs=[]\n",
    "        lossesT=[]\n",
    "        lossesV=[]\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch+1,max_epochs))\n",
    "            print('-'*10)\n",
    "            \n",
    "            since=time.time()\n",
    "            train_metrics=defaultdict(float)\n",
    "            total_loss=0\n",
    "            running_corrects=0\n",
    "            num_steps=0\n",
    "            \n",
    "            all_labels_d = torch.tensor([], dtype=torch.long).to(device)\n",
    "            all_predictions_d = torch.tensor([], dtype=torch.long).to(device)\n",
    "            all_predictions_probabilities_d = torch.tensor([], dtype=torch.float).to(device)\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            #Training\n",
    "            for image, labels in self.training_generator:\n",
    "                #Transfer to GPU:\n",
    "                \n",
    "                image, labels = image.to(device, dtype=torch.float32), labels.to(device)\n",
    "                outputs = model(image)\n",
    "                predicted_probability, predicted  = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "              \n",
    "                num_steps+=image.size(0)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss+=loss.item()*image.size(0)\n",
    "           \n",
    "                running_corrects += torch.sum(predicted == labels.data)\n",
    "                all_labels_d = torch.cat((all_labels_d, labels), 0)\n",
    "                all_predictions_d = torch.cat((all_predictions_d, predicted), 0)\n",
    "                all_predictions_probabilities_d = torch.cat((all_predictions_probabilities_d, predicted_probability), 0)\n",
    "                \n",
    "                \n",
    "            y_true = all_labels_d.cpu()\n",
    "            y_predicted = all_predictions_d.cpu()  # to('cpu')\n",
    "            valset_predicted_probabilites = all_predictions_probabilities_d.cpu()  # to('cpu')\n",
    "            \n",
    "            \n",
    "            #############################\n",
    "            # Standard metrics \n",
    "            #############################\n",
    "        \n",
    "            train_micro_precision=mtc.precision_score(y_true, y_predicted, average=\"micro\")     \n",
    "            train_micro_recall=mtc.recall_score(y_true, y_predicted, average=\"micro\")\n",
    "            train_micro_f1=mtc.f1_score(y_true, y_predicted, average=\"micro\")  \n",
    "        \n",
    "            train_macro_precision=mtc.precision_score(y_true, y_predicted, average=\"macro\")     \n",
    "            train_macro_recall=mtc.recall_score(y_true, y_predicted, average=\"macro\")\n",
    "            train_macro_f1=mtc.f1_score(y_true, y_predicted, average=\"macro\")  \n",
    "        \n",
    "            train_mcc=mtc.matthews_corrcoef(y_true, y_predicted)\n",
    "             \n",
    "            \n",
    "            train_metrics['loss']=total_loss/num_steps\n",
    "        \n",
    "            train_metrics['micro_precision']=train_micro_precision\n",
    "            train_metrics['micro_recall']=train_micro_recall\n",
    "            train_metrics['micro_f1']=train_micro_f1\n",
    "            train_metrics['macro_precision']=train_macro_precision\n",
    "            train_metrics['macro_recall']=train_macro_recall\n",
    "            train_metrics['macro_f1']=train_macro_f1\n",
    "            train_metrics['mcc']=train_mcc\n",
    "            \n",
    "            print('Training...')\n",
    "            print('Train_loss:{:.3f}'.format(total_loss/num_steps))\n",
    "           \n",
    "            \n",
    "            print_metrics(train_metrics,num_steps)\n",
    "\n",
    "            ############################\n",
    "            # Validation\n",
    "            ############################\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_metrics, val_num_steps=validate_net(model,self.validation_generator,device,criterion)\n",
    "                \n",
    "            scheduler.step(val_loss)\n",
    "            epochs.append(epoch)\n",
    "            lossesT.append(total_loss/num_steps)\n",
    "            lossesV.append(val_loss)\n",
    "            \n",
    "            print('.'*5)\n",
    "            print('Validating...')\n",
    "            print('val_loss:{:.3f}'.format(val_loss))\n",
    "        \n",
    "            print_metrics(val_metrics,val_num_steps)\n",
    "\n",
    "\n",
    "            ##################################################################\n",
    "            # Writing epoch-wise training and validation results to a csv file \n",
    "            ##################################################################\n",
    "\n",
    "            key_name=['Epoch','Train_loss','Train_micro_precision','Train_micro_recall','Train_micro_f1','Train_macro_precision','Train_macro_recall','Train_macro_f1','Train_mcc','Val_loss','Val_micro_precision','Val_micro_recall','Val_micro_f1','Val_macro_precision','Val_macro_recall','Val_macro_f1','Val_mcc']\n",
    "            train_list=[]\n",
    "            train_list.append(epoch)\n",
    "\n",
    "            try:\n",
    "\n",
    "                with open(filename+str('.csv'), 'a',newline=\"\") as f:\n",
    "                    wr = csv.writer(f,delimiter=\",\")\n",
    "                    if epoch==0:\n",
    "                        wr.writerow(key_name)\n",
    "\n",
    "                    for k, vl in train_metrics.items():\n",
    "                        train_list.append(vl)\n",
    "\n",
    "                    train_list.append(val_loss)\n",
    "\n",
    "                    for k, vl in val_metrics.items():\n",
    "                        train_list.append(vl)\n",
    "                    zip(train_list)\n",
    "                    wr.writerow(train_list)\n",
    "\n",
    "\n",
    "            except IOError:\n",
    "                print(\"I/O Error\")\n",
    "\n",
    "            \n",
    "            ##############################\n",
    "            # Saving best model \n",
    "            ##############################\n",
    "            \n",
    "            if val_metrics['micro_f1']>=val_f1_max:\n",
    "                print('val micro f1 increased ({:.6f}-->{:.6f}).Saving model'.format(val_f1_max,val_metrics['micro_f1']))\n",
    "                \n",
    "                torch.save({'epoch':epoch+1,\n",
    "                            'model_state_dict':model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scheduler': scheduler.state_dict(), \n",
    "                            'loss':val_loss},model_path+f'/C_{epoch+1}_{batch_size}.pth')\n",
    "                best_model_path=model_path+f'/C_{epoch+1}_{batch_size}.pth'\n",
    "               \n",
    "                val_f1_max=val_metrics['micro_f1']\n",
    "                \n",
    "\n",
    "            print('-'*10)\n",
    "       \n",
    "        time_elapsed=time.time()-since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "        training_curve(epochs,lossesT,lossesV)\n",
    "        epochs.clear()\n",
    "        lossesT.clear()\n",
    "        lossesV.clear()\n",
    "        \n",
    "\n",
    "        ############################\n",
    "        #         Test\n",
    "        ############################\n",
    "        test_list=[]\n",
    "        print('Best model path:{}'.format(best_model_path))\n",
    "        best_model=torchvision.models.densenet121(weights=False).to(device)\n",
    "        \n",
    "        n_inputs = best_model.classifier.in_features\n",
    "        best_model.classifier = nn.Sequential(\n",
    "                      nn.Linear(n_inputs, n_classes),               \n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    " \n",
    "        checkpoint=torch.load(best_model_path,map_location=device)   # loading best model\n",
    "        best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model.to(device)\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "       \t       test_loss, test_metrics, test_num_steps=test_net(best_model,self.test_generator,device,criterion)\n",
    "\n",
    "        \n",
    "        print_metrics(test_metrics,test_num_steps)\n",
    "        test_list.append(test_loss)\n",
    "     \n",
    "\n",
    "        for k, vl in test_metrics.items():      \n",
    "            test_list.append(vl)              # append metrics results in a list\n",
    "  \n",
    "  \n",
    "  \n",
    "        ##################################################################\n",
    "        # Writing test results to a csv file \n",
    "        ##################################################################\n",
    "\n",
    "        key_name=['Test_loss','Test_micro_precision','Test_micro_recall','Test_micro_f1','Test_macro_precision','Test_macro_recall','Test_macro_f1','Test_mcc']\n",
    "        try:\n",
    "\n",
    "                with open(filename+str('.csv'), 'a',newline=\"\") as f:\n",
    "                    wr = csv.writer(f,delimiter=\",\")\n",
    "                    wr.writerow(key_name)\n",
    "                    zip(test_list)\n",
    "                    wr.writerow(test_list) \n",
    "                    wr.writerow(\"\") \n",
    "        except IOError:\n",
    "                print(\"I/O Error\")  \n",
    "        return val_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435934c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logging.info(f'Using device: {device}')\n",
    "logging.info(f'''Starting training:\n",
    "             Epochs: {max_epochs}\n",
    "             Batch Size: {batch_size}\n",
    "             Learning Rate: {lr}''')\n",
    "t=train()\n",
    "t.train_net()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
